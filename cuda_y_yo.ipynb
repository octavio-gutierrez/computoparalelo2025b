{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install pynvjitlink\n",
        "\n",
        "#import pynvjitlink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzXb9TosvFa3",
        "outputId": "f699a906-9bdc-49b3-aa6d-9048dc7db257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pynvjitlink (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pynvjitlink\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvjitlink-cu12\n",
        "\n",
        "import pynvjitlink\n",
        "\n",
        "from numba import config\n",
        "\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQCg5yivLrU",
        "outputId": "4782e7b3-456e-4f27-9a03-438550ae7bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynvjitlink-cu12\n",
            "  Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvjitlink-cu12\n",
            "Successfully installed pynvjitlink-cu12-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --system numba-cuda==0.4.0\n",
        "from numba import config\n",
        "\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "HrGiIXgxijM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzLwRlL523US",
        "outputId": "f3a237ed-7f30-4829-e689-1d7318a76a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CUDA devices\n",
            "id 0             b'Tesla T4'                              [SUPPORTED]\n",
            "                      Compute Capability: 7.5\n",
            "                           PCI Device ID: 4\n",
            "                              PCI Bus ID: 0\n",
            "                                    UUID: GPU-03bc8767-60a1-e1d0-ac06-631224108fbb\n",
            "                                Watchdog: Disabled\n",
            "             FP32/FP64 Performance Ratio: 32\n",
            "Summary:\n",
            "\t1/1 devices are supported\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weakproxy at 0x78774d003f10 to Device at 0x78774e02d5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "cuda.gpus\n",
        "cuda.detect()\n",
        "cuda.select_device(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "cc_cores_per_SM_dict = {  (2,0) : 32,  (2,1) : 48, (3,0) : 192, (3,5) : 192, (3,7) : 192, (5,0) : 128, (5,2) : 128, (6,0) : 64, (6,1) : 128, (7,0) : 64, (7,5) : 64, (8,0) : 64, (8,6) : 128, (8,9) : 128, (9,0) : 128 }\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "sms = getattr(device, \"MULTIPROCESSOR_COUNT\")\n",
        "cc = device.compute_capability\n",
        "cores_por_sm = cc_cores_per_SM_dict.get(cc)\n",
        "cores_totales = cores_por_sm * sms\n",
        "print(\"GPU compute capability: \", cc)\n",
        "print(\"Número de SM: \", sms)\n",
        "print(\"Número de cores: \", cores_totales)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8wBOlF76WUk",
        "outputId": "4c270054-7620-495d-a9b8-8da09ec51058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU compute capability:  (7, 5)\n",
            "Número de SM:  40\n",
            "Número de cores:  2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba.cuda.cudadrv import enums\n",
        "from numba import cuda\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "attribs = [name.replace(\"CU_DEVICE_ATTRIBUTE_\", \"\") for name in dir(enums) if name.startswith(\"CU_DEVICE_ATTRIBUTE_\")]\n",
        "for attr in attribs:\n",
        "    print(attr, \"=\", getattr(device, attr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf1whkyo7csO",
        "outputId": "50b10f66-c42c-4852-c53e-483c08eba59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASYNC_ENGINE_COUNT = 3\n",
            "CAN_MAP_HOST_MEMORY = 1\n",
            "CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM = 1\n",
            "CLOCK_RATE = 1590000\n",
            "COMPUTE_CAPABILITY_MAJOR = 7\n",
            "COMPUTE_CAPABILITY_MINOR = 5\n",
            "COMPUTE_MODE = 0\n",
            "COMPUTE_PREEMPTION_SUPPORTED = 1\n",
            "CONCURRENT_KERNELS = 1\n",
            "CONCURRENT_MANAGED_ACCESS = 1\n",
            "COOPERATIVE_LAUNCH = 1\n",
            "COOPERATIVE_MULTI_DEVICE_LAUNCH = 1\n",
            "ECC_ENABLED = 1\n",
            "GLOBAL_L1_CACHE_SUPPORTED = 1\n",
            "GLOBAL_MEMORY_BUS_WIDTH = 256\n",
            "GPU_OVERLAP = 1\n",
            "HOST_NATIVE_ATOMIC_SUPPORTED = 0\n",
            "INTEGRATED = 0\n",
            "IS_MULTI_GPU_BOARD = 0\n",
            "KERNEL_EXEC_TIMEOUT = 0\n",
            "L2_CACHE_SIZE = 4194304\n",
            "LOCAL_L1_CACHE_SUPPORTED = 1\n",
            "MANAGED_MEMORY = 1\n",
            "MAX_BLOCK_DIM_X = 1024\n",
            "MAX_BLOCK_DIM_Y = 1024\n",
            "MAX_BLOCK_DIM_Z = 64\n",
            "MAX_GRID_DIM_X = 2147483647\n",
            "MAX_GRID_DIM_Y = 65535\n",
            "MAX_GRID_DIM_Z = 65535\n",
            "MAX_MAX_TEXTURE_2D_MIPMAPPED_HEIGHT = 32768\n",
            "MAX_PITCH = 2147483647\n",
            "MAX_REGISTERS_PER_BLOCK = 65536\n",
            "MAX_REGISTERS_PER_MULTIPROCESSOR = 65536\n",
            "MAX_SHARED_MEMORY_PER_BLOCK = 49152\n",
            "MAX_SHARED_MEMORY_PER_BLOCK_OPTIN = 65536\n",
            "MAX_SHARED_MEMORY_PER_MULTIPROCESSOR = 65536\n",
            "MAX_SURFACE_1D_LAYERED_LAYERS = 2048\n",
            "MAX_SURFACE_1D_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_1D_WIDTH = 32768\n",
            "MAX_SURFACE_2D_HEIGHT = 65536\n",
            "MAX_SURFACE_2D_LAYERED_HEIGHT = 32768\n",
            "MAX_SURFACE_2D_LAYERED_LAYERS = 2048\n",
            "MAX_SURFACE_2D_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_2D_WIDTH = 131072\n",
            "MAX_SURFACE_3D_DEPTH = 16384\n",
            "MAX_SURFACE_3D_HEIGHT = 16384\n",
            "MAX_SURFACE_3D_WIDTH = 16384\n",
            "MAX_SURFACE_CUBEMAP_LAYERED_LAYERS = 2046\n",
            "MAX_SURFACE_CUBEMAP_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_CUBEMAP_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_LAYERED_LAYERS = 2048\n",
            "MAX_TEXTURE_1D_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_LINEAR_WIDTH = 268435456\n",
            "MAX_TEXTURE_1D_MIPMAPPED_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_WIDTH = 131072\n",
            "MAX_TEXTURE_2D_GATHER_HEIGHT = 32768\n",
            "MAX_TEXTURE_2D_GATHER_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_HEIGHT = 65536\n",
            "MAX_TEXTURE_2D_LAYERED_HEIGHT = 32768\n",
            "MAX_TEXTURE_2D_LAYERED_LAYERS = 2048\n",
            "MAX_TEXTURE_2D_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_LINEAR_HEIGHT = 65000\n",
            "MAX_TEXTURE_2D_LINEAR_PITCH = 2097120\n",
            "MAX_TEXTURE_2D_LINEAR_WIDTH = 131072\n",
            "MAX_TEXTURE_2D_MIPMAPPED_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_WIDTH = 131072\n",
            "MAX_TEXTURE_3D_DEPTH = 16384\n",
            "MAX_TEXTURE_3D_DEPTH_ALT = 32768\n",
            "MAX_TEXTURE_3D_HEIGHT = 16384\n",
            "MAX_TEXTURE_3D_HEIGHT_ALT = 8192\n",
            "MAX_TEXTURE_3D_WIDTH = 16384\n",
            "MAX_TEXTURE_3D_WIDTH_ALT = 8192\n",
            "MAX_TEXTURE_CUBEMAP_LAYERED_LAYERS = 2046\n",
            "MAX_TEXTURE_CUBEMAP_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_CUBEMAP_WIDTH = 32768\n",
            "MAX_THREADS_PER_BLOCK = 1024\n",
            "MAX_THREADS_PER_MULTI_PROCESSOR = 1024\n",
            "MEMORY_CLOCK_RATE = 5001000\n",
            "MULTIPROCESSOR_COUNT = 40\n",
            "MULTI_GPU_BOARD_GROUP_ID = 0\n",
            "PAGEABLE_MEMORY_ACCESS = 0\n",
            "PCI_BUS_ID = 0\n",
            "PCI_DEVICE_ID = 4\n",
            "PCI_DOMAIN_ID = 0\n",
            "SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO = 32\n",
            "STREAM_PRIORITIES_SUPPORTED = 1\n",
            "SURFACE_ALIGNMENT = 512\n",
            "TCC_DRIVER = 0\n",
            "TEXTURE_ALIGNMENT = 512\n",
            "TEXTURE_PITCH_ALIGNMENT = 32\n",
            "TOTAL_CONSTANT_MEMORY = 65536\n",
            "UNIFIED_ADDRESSING = 1\n",
            "WARP_SIZE = 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Lanzamiento de un kernel\n",
        "\n"
      ],
      "metadata": {
        "id": "FFWuhpqOdj9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "@cuda.jit\n",
        "def un_kernel(x, y, out):\n",
        "  pass\n",
        "\n",
        "x = np.ones(256)\n",
        "y = x * 2\n",
        "out = np.empty_like(x)\n",
        "block_size = 32\n",
        "num_blocks = 1000\n",
        "un_kernel[num_blocks, block_size](x, y, out)\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_fLMkcedgoL",
        "outputId": "6f3c799d-6986-4590-ddaf-6bcf1de1e326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.54410941e-310 6.54410941e-310 3.84335840e-315 3.84335840e-315\n",
            " 4.94065646e-324 3.84335871e-315 3.84336030e-315 4.94065646e-324\n",
            " 3.84335883e-315 3.84335887e-315 1.18575755e-322 6.54410904e-310\n",
            " 9.88131292e-324 3.84335927e-315 3.45845952e-323 6.36598738e-314\n",
            " 1.18575755e-322 6.54410757e-310 2.97079411e-313 3.18299369e-313\n",
            " 1.27319747e-313 1.69759663e-313 1.03753786e-322 3.84335911e-315\n",
            " 6.54410783e-310 6.54402282e-310 1.27319747e-313 4.03179200e-313\n",
            " 4.94065646e-324 3.84335966e-315 3.84335970e-315 1.18575755e-322\n",
            " 5.78950076e-317 1.90979621e-313 2.96439388e-323 4.24399158e-313\n",
            " 4.45619116e-313 0.00000000e+000 3.84336002e-315 0.00000000e+000\n",
            " 8.39911598e-323 3.84335935e-315 3.84335958e-315 3.84335994e-315\n",
            " 1.27319747e-313 4.66839074e-313 2.96439388e-323 3.84335875e-315\n",
            " 3.84336006e-315 0.00000000e+000 1.03753786e-322 3.84335998e-315\n",
            " 6.54410907e-310 4.94065646e-324 3.45845952e-323 4.66839074e-313\n",
            " 0.00000000e+000 3.84336077e-315 0.00000000e+000 4.94065646e-324\n",
            " 3.84335864e-315 3.84336069e-315 1.18575755e-322 6.54410907e-310\n",
            " 4.94065646e-324 3.84335982e-315 2.12199579e-314 1.06099790e-313\n",
            " 1.03753786e-322 3.84336093e-315 6.54410907e-310 4.94065646e-324\n",
            " 2.12199579e-314 1.90979621e-313 0.00000000e+000 3.84336148e-315\n",
            " 0.00000000e+000 4.94065646e-324 6.54408653e-310 3.84336002e-315\n",
            " 3.84336030e-315 3.84336081e-315 0.00000000e+000 0.00000000e+000\n",
            " 3.84336140e-315 9.88131292e-324 1.27319747e-313 0.00000000e+000\n",
            " 3.84336199e-315 0.00000000e+000 6.54402282e-310 3.84335864e-315\n",
            " 3.84336192e-315 8.48798316e-314 2.75859453e-313 1.18575755e-322\n",
            " 5.78950076e-317 4.94065646e-324 3.84336192e-315 2.97079411e-313\n",
            " 3.18299369e-313 5.78950076e-317 0.00000000e+000 0.00000000e+000\n",
            " 2.97079411e-313 3.18299369e-313 1.18575755e-322 5.78952448e-317\n",
            " 4.94065646e-324 1.48539705e-313 3.60739284e-313 3.81959242e-313\n",
            " 5.78952448e-317 0.00000000e+000 0.00000000e+000 3.60739284e-313\n",
            " 3.81959242e-313 1.18575755e-322 6.54410904e-310 1.48539705e-313\n",
            " 2.71615461e-312 4.24399158e-313 4.88059032e-313 6.54410904e-310\n",
            " 0.00000000e+000 0.00000000e+000 4.24399158e-313 4.88059032e-313\n",
            " 1.18575755e-322 6.54410904e-310 4.94065646e-324 1.48539705e-313\n",
            " 4.24399158e-313 4.88059032e-313 6.54410904e-310 0.00000000e+000\n",
            " 0.00000000e+000 4.24399158e-313 4.88059032e-313 1.48219694e-323\n",
            " 3.84336405e-315 3.84336247e-315 3.84336290e-315 3.84336377e-315\n",
            " 1.18575755e-322 5.78950076e-317 4.94065646e-324 3.84336401e-315\n",
            " 2.97079411e-313 3.18299369e-313 5.78950076e-317 0.00000000e+000\n",
            " 0.00000000e+000 2.97079411e-313 3.18299369e-313 1.18575755e-322\n",
            " 5.78952448e-317 4.94065646e-324 5.57306049e-321 3.60739284e-313\n",
            " 3.81959242e-313 5.78952448e-317 0.00000000e+000 0.00000000e+000\n",
            " 3.60739284e-313 3.81959242e-313 1.18575755e-322 6.54410904e-310\n",
            " 4.94065646e-324 3.84336488e-315 4.24399158e-313 4.88059032e-313\n",
            " 6.54410904e-310 0.00000000e+000 0.00000000e+000 4.24399158e-313\n",
            " 4.88059032e-313 1.18575755e-322 6.54410904e-310 4.94065646e-324\n",
            " 4.94065646e-324 4.24399158e-313 4.88059032e-313 6.54410904e-310\n",
            " 0.00000000e+000 0.00000000e+000 4.24399158e-313 4.88059032e-313\n",
            " 1.48219694e-323 3.84336599e-315 3.84336441e-315 3.84336484e-315\n",
            " 3.84336571e-315 1.18575755e-322 5.78950076e-317 4.88059032e-313\n",
            " 5.56317917e-321 2.97079411e-313 3.18299369e-313 5.78950076e-317\n",
            " 0.00000000e+000 0.00000000e+000 2.97079411e-313 3.18299369e-313\n",
            " 1.18575755e-322 5.78952448e-317 4.94065646e-324 3.84336638e-315\n",
            " 3.60739284e-313 3.81959242e-313 5.78952448e-317 0.00000000e+000\n",
            " 0.00000000e+000 3.60739284e-313 3.81959242e-313 1.18575755e-322\n",
            " 6.54410904e-310 4.94065646e-324 8.27578359e-313 4.24399158e-313\n",
            " 4.88059032e-313 6.54410904e-310 0.00000000e+000 0.00000000e+000\n",
            " 4.24399158e-313 4.88059032e-313 1.18575755e-322 6.54410904e-310\n",
            " 5.09278990e-313 9.76118064e-313 4.24399158e-313 4.88059032e-313\n",
            " 6.54410904e-310 0.00000000e+000 0.00000000e+000 4.24399158e-313\n",
            " 4.88059032e-313 1.48219694e-323 3.84336792e-315 3.84336634e-315\n",
            " 3.84336678e-315 3.84336765e-315 1.18575755e-322 5.78950076e-317\n",
            " 4.94065646e-324 5.49895064e-321 2.97079411e-313 3.18299369e-313\n",
            " 5.78950076e-317 0.00000000e+000 0.00000000e+000 2.97079411e-313\n",
            " 3.18299369e-313 1.18575755e-322 5.78952448e-317 4.94065646e-324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:893: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel minimalista con posicion absoluta manual"
      ],
      "metadata": {
        "id": "ldiw0edqlP9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def kernel_doblador(arreglo, log):\n",
        "\n",
        "  posicion = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "\n",
        "  if posicion < arreglo.size:\n",
        "    arreglo[posicion] *= 2\n",
        "    log[posicion][0] = posicion\n",
        "    log[posicion][1] = cuda.threadIdx.x\n",
        "    log[posicion][2] = cuda.blockIdx.x\n",
        "    log[posicion][3] = cuda.blockDim.x\n",
        "\n",
        "\n",
        "x = np.ones(256)\n",
        "log = np.zeros(shape=(x.size, 4))\n",
        "\n",
        "block_size = 32\n",
        "num_blocks = 10\n",
        "kernel_doblador[num_blocks, block_size](x, log)\n",
        "print(x)\n",
        "for record in log:\n",
        "  print(record)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQdJFxX_lPMz",
        "outputId": "89d400da-aeea-4cbb-f7d7-79ab998499ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 10 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            "[ 0.  0.  0. 32.]\n",
            "[ 1.  1.  0. 32.]\n",
            "[ 2.  2.  0. 32.]\n",
            "[ 3.  3.  0. 32.]\n",
            "[ 4.  4.  0. 32.]\n",
            "[ 5.  5.  0. 32.]\n",
            "[ 6.  6.  0. 32.]\n",
            "[ 7.  7.  0. 32.]\n",
            "[ 8.  8.  0. 32.]\n",
            "[ 9.  9.  0. 32.]\n",
            "[10. 10.  0. 32.]\n",
            "[11. 11.  0. 32.]\n",
            "[12. 12.  0. 32.]\n",
            "[13. 13.  0. 32.]\n",
            "[14. 14.  0. 32.]\n",
            "[15. 15.  0. 32.]\n",
            "[16. 16.  0. 32.]\n",
            "[17. 17.  0. 32.]\n",
            "[18. 18.  0. 32.]\n",
            "[19. 19.  0. 32.]\n",
            "[20. 20.  0. 32.]\n",
            "[21. 21.  0. 32.]\n",
            "[22. 22.  0. 32.]\n",
            "[23. 23.  0. 32.]\n",
            "[24. 24.  0. 32.]\n",
            "[25. 25.  0. 32.]\n",
            "[26. 26.  0. 32.]\n",
            "[27. 27.  0. 32.]\n",
            "[28. 28.  0. 32.]\n",
            "[29. 29.  0. 32.]\n",
            "[30. 30.  0. 32.]\n",
            "[31. 31.  0. 32.]\n",
            "[32.  0.  1. 32.]\n",
            "[33.  1.  1. 32.]\n",
            "[34.  2.  1. 32.]\n",
            "[35.  3.  1. 32.]\n",
            "[36.  4.  1. 32.]\n",
            "[37.  5.  1. 32.]\n",
            "[38.  6.  1. 32.]\n",
            "[39.  7.  1. 32.]\n",
            "[40.  8.  1. 32.]\n",
            "[41.  9.  1. 32.]\n",
            "[42. 10.  1. 32.]\n",
            "[43. 11.  1. 32.]\n",
            "[44. 12.  1. 32.]\n",
            "[45. 13.  1. 32.]\n",
            "[46. 14.  1. 32.]\n",
            "[47. 15.  1. 32.]\n",
            "[48. 16.  1. 32.]\n",
            "[49. 17.  1. 32.]\n",
            "[50. 18.  1. 32.]\n",
            "[51. 19.  1. 32.]\n",
            "[52. 20.  1. 32.]\n",
            "[53. 21.  1. 32.]\n",
            "[54. 22.  1. 32.]\n",
            "[55. 23.  1. 32.]\n",
            "[56. 24.  1. 32.]\n",
            "[57. 25.  1. 32.]\n",
            "[58. 26.  1. 32.]\n",
            "[59. 27.  1. 32.]\n",
            "[60. 28.  1. 32.]\n",
            "[61. 29.  1. 32.]\n",
            "[62. 30.  1. 32.]\n",
            "[63. 31.  1. 32.]\n",
            "[64.  0.  2. 32.]\n",
            "[65.  1.  2. 32.]\n",
            "[66.  2.  2. 32.]\n",
            "[67.  3.  2. 32.]\n",
            "[68.  4.  2. 32.]\n",
            "[69.  5.  2. 32.]\n",
            "[70.  6.  2. 32.]\n",
            "[71.  7.  2. 32.]\n",
            "[72.  8.  2. 32.]\n",
            "[73.  9.  2. 32.]\n",
            "[74. 10.  2. 32.]\n",
            "[75. 11.  2. 32.]\n",
            "[76. 12.  2. 32.]\n",
            "[77. 13.  2. 32.]\n",
            "[78. 14.  2. 32.]\n",
            "[79. 15.  2. 32.]\n",
            "[80. 16.  2. 32.]\n",
            "[81. 17.  2. 32.]\n",
            "[82. 18.  2. 32.]\n",
            "[83. 19.  2. 32.]\n",
            "[84. 20.  2. 32.]\n",
            "[85. 21.  2. 32.]\n",
            "[86. 22.  2. 32.]\n",
            "[87. 23.  2. 32.]\n",
            "[88. 24.  2. 32.]\n",
            "[89. 25.  2. 32.]\n",
            "[90. 26.  2. 32.]\n",
            "[91. 27.  2. 32.]\n",
            "[92. 28.  2. 32.]\n",
            "[93. 29.  2. 32.]\n",
            "[94. 30.  2. 32.]\n",
            "[95. 31.  2. 32.]\n",
            "[96.  0.  3. 32.]\n",
            "[97.  1.  3. 32.]\n",
            "[98.  2.  3. 32.]\n",
            "[99.  3.  3. 32.]\n",
            "[100.   4.   3.  32.]\n",
            "[101.   5.   3.  32.]\n",
            "[102.   6.   3.  32.]\n",
            "[103.   7.   3.  32.]\n",
            "[104.   8.   3.  32.]\n",
            "[105.   9.   3.  32.]\n",
            "[106.  10.   3.  32.]\n",
            "[107.  11.   3.  32.]\n",
            "[108.  12.   3.  32.]\n",
            "[109.  13.   3.  32.]\n",
            "[110.  14.   3.  32.]\n",
            "[111.  15.   3.  32.]\n",
            "[112.  16.   3.  32.]\n",
            "[113.  17.   3.  32.]\n",
            "[114.  18.   3.  32.]\n",
            "[115.  19.   3.  32.]\n",
            "[116.  20.   3.  32.]\n",
            "[117.  21.   3.  32.]\n",
            "[118.  22.   3.  32.]\n",
            "[119.  23.   3.  32.]\n",
            "[120.  24.   3.  32.]\n",
            "[121.  25.   3.  32.]\n",
            "[122.  26.   3.  32.]\n",
            "[123.  27.   3.  32.]\n",
            "[124.  28.   3.  32.]\n",
            "[125.  29.   3.  32.]\n",
            "[126.  30.   3.  32.]\n",
            "[127.  31.   3.  32.]\n",
            "[128.   0.   4.  32.]\n",
            "[129.   1.   4.  32.]\n",
            "[130.   2.   4.  32.]\n",
            "[131.   3.   4.  32.]\n",
            "[132.   4.   4.  32.]\n",
            "[133.   5.   4.  32.]\n",
            "[134.   6.   4.  32.]\n",
            "[135.   7.   4.  32.]\n",
            "[136.   8.   4.  32.]\n",
            "[137.   9.   4.  32.]\n",
            "[138.  10.   4.  32.]\n",
            "[139.  11.   4.  32.]\n",
            "[140.  12.   4.  32.]\n",
            "[141.  13.   4.  32.]\n",
            "[142.  14.   4.  32.]\n",
            "[143.  15.   4.  32.]\n",
            "[144.  16.   4.  32.]\n",
            "[145.  17.   4.  32.]\n",
            "[146.  18.   4.  32.]\n",
            "[147.  19.   4.  32.]\n",
            "[148.  20.   4.  32.]\n",
            "[149.  21.   4.  32.]\n",
            "[150.  22.   4.  32.]\n",
            "[151.  23.   4.  32.]\n",
            "[152.  24.   4.  32.]\n",
            "[153.  25.   4.  32.]\n",
            "[154.  26.   4.  32.]\n",
            "[155.  27.   4.  32.]\n",
            "[156.  28.   4.  32.]\n",
            "[157.  29.   4.  32.]\n",
            "[158.  30.   4.  32.]\n",
            "[159.  31.   4.  32.]\n",
            "[160.   0.   5.  32.]\n",
            "[161.   1.   5.  32.]\n",
            "[162.   2.   5.  32.]\n",
            "[163.   3.   5.  32.]\n",
            "[164.   4.   5.  32.]\n",
            "[165.   5.   5.  32.]\n",
            "[166.   6.   5.  32.]\n",
            "[167.   7.   5.  32.]\n",
            "[168.   8.   5.  32.]\n",
            "[169.   9.   5.  32.]\n",
            "[170.  10.   5.  32.]\n",
            "[171.  11.   5.  32.]\n",
            "[172.  12.   5.  32.]\n",
            "[173.  13.   5.  32.]\n",
            "[174.  14.   5.  32.]\n",
            "[175.  15.   5.  32.]\n",
            "[176.  16.   5.  32.]\n",
            "[177.  17.   5.  32.]\n",
            "[178.  18.   5.  32.]\n",
            "[179.  19.   5.  32.]\n",
            "[180.  20.   5.  32.]\n",
            "[181.  21.   5.  32.]\n",
            "[182.  22.   5.  32.]\n",
            "[183.  23.   5.  32.]\n",
            "[184.  24.   5.  32.]\n",
            "[185.  25.   5.  32.]\n",
            "[186.  26.   5.  32.]\n",
            "[187.  27.   5.  32.]\n",
            "[188.  28.   5.  32.]\n",
            "[189.  29.   5.  32.]\n",
            "[190.  30.   5.  32.]\n",
            "[191.  31.   5.  32.]\n",
            "[192.   0.   6.  32.]\n",
            "[193.   1.   6.  32.]\n",
            "[194.   2.   6.  32.]\n",
            "[195.   3.   6.  32.]\n",
            "[196.   4.   6.  32.]\n",
            "[197.   5.   6.  32.]\n",
            "[198.   6.   6.  32.]\n",
            "[199.   7.   6.  32.]\n",
            "[200.   8.   6.  32.]\n",
            "[201.   9.   6.  32.]\n",
            "[202.  10.   6.  32.]\n",
            "[203.  11.   6.  32.]\n",
            "[204.  12.   6.  32.]\n",
            "[205.  13.   6.  32.]\n",
            "[206.  14.   6.  32.]\n",
            "[207.  15.   6.  32.]\n",
            "[208.  16.   6.  32.]\n",
            "[209.  17.   6.  32.]\n",
            "[210.  18.   6.  32.]\n",
            "[211.  19.   6.  32.]\n",
            "[212.  20.   6.  32.]\n",
            "[213.  21.   6.  32.]\n",
            "[214.  22.   6.  32.]\n",
            "[215.  23.   6.  32.]\n",
            "[216.  24.   6.  32.]\n",
            "[217.  25.   6.  32.]\n",
            "[218.  26.   6.  32.]\n",
            "[219.  27.   6.  32.]\n",
            "[220.  28.   6.  32.]\n",
            "[221.  29.   6.  32.]\n",
            "[222.  30.   6.  32.]\n",
            "[223.  31.   6.  32.]\n",
            "[224.   0.   7.  32.]\n",
            "[225.   1.   7.  32.]\n",
            "[226.   2.   7.  32.]\n",
            "[227.   3.   7.  32.]\n",
            "[228.   4.   7.  32.]\n",
            "[229.   5.   7.  32.]\n",
            "[230.   6.   7.  32.]\n",
            "[231.   7.   7.  32.]\n",
            "[232.   8.   7.  32.]\n",
            "[233.   9.   7.  32.]\n",
            "[234.  10.   7.  32.]\n",
            "[235.  11.   7.  32.]\n",
            "[236.  12.   7.  32.]\n",
            "[237.  13.   7.  32.]\n",
            "[238.  14.   7.  32.]\n",
            "[239.  15.   7.  32.]\n",
            "[240.  16.   7.  32.]\n",
            "[241.  17.   7.  32.]\n",
            "[242.  18.   7.  32.]\n",
            "[243.  19.   7.  32.]\n",
            "[244.  20.   7.  32.]\n",
            "[245.  21.   7.  32.]\n",
            "[246.  22.   7.  32.]\n",
            "[247.  23.   7.  32.]\n",
            "[248.  24.   7.  32.]\n",
            "[249.  25.   7.  32.]\n",
            "[250.  26.   7.  32.]\n",
            "[251.  27.   7.  32.]\n",
            "[252.  28.   7.  32.]\n",
            "[253.  29.   7.  32.]\n",
            "[254.  30.   7.  32.]\n",
            "[255.  31.   7.  32.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:893: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid de menor tamaño que la estructura de datos"
      ],
      "metadata": {
        "id": "J0DyxJ2crE3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def kernel_doblador(arreglo):\n",
        "  #posicion = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "  posicion = cuda.grid(1)\n",
        "  #tamano_grid = cuda.blockDim.x * cuda.gridDim.x\n",
        "  tamano_grid = cuda.gridsize(1)\n",
        "  # for (i = posicion; i < arreglo.size; i+=tamano_grid)\n",
        "  for i in range(posicion, arreglo.size, tamano_grid):\n",
        "    arreglo[i] *= 2\n",
        "\n",
        "\n",
        "x = np.ones(128)\n",
        "\n",
        "block_size = 32\n",
        "num_blocks = 2\n",
        "kernel_doblador[num_blocks, block_size](x)\n",
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92gBjDKgrDwg",
        "outputId": "36727473-f41c-4332-e09d-cb6b6a304d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:893: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid ajustable a estructura de datos"
      ],
      "metadata": {
        "id": "KzoJ5u-izzdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "@cuda.jit\n",
        "def kernel_doblador(x, y, out):\n",
        "  posicion = cuda.grid(1)\n",
        "  tamano_grid = cuda.gridsize(1)\n",
        "  if posicion < x.size:\n",
        "    out[posicion] = x[posicion] + y[posicion]\n",
        "\n",
        "\n",
        "x = np.arange(500)\n",
        "y = x * 2\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "out = np.empty_like(x)\n",
        "out_device = cuda.device_array_like(x)\n",
        "\n",
        "block_size = 32\n",
        "num_blocks =  int(math.ceil(x.size / block_size))\n",
        "kernel_doblador[num_blocks, block_size](x_device, y_device, out_device)\n",
        "out = out_device.copy_to_host()\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9liMpH-IzzJW",
        "outputId": "b475c91f-32b7-4eb9-a2c9-d0d405c4d93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    3    6    9   12   15   18   21   24   27   30   33   36   39\n",
            "   42   45   48   51   54   57   60   63   66   69   72   75   78   81\n",
            "   84   87   90   93   96   99  102  105  108  111  114  117  120  123\n",
            "  126  129  132  135  138  141  144  147  150  153  156  159  162  165\n",
            "  168  171  174  177  180  183  186  189  192  195  198  201  204  207\n",
            "  210  213  216  219  222  225  228  231  234  237  240  243  246  249\n",
            "  252  255  258  261  264  267  270  273  276  279  282  285  288  291\n",
            "  294  297  300  303  306  309  312  315  318  321  324  327  330  333\n",
            "  336  339  342  345  348  351  354  357  360  363  366  369  372  375\n",
            "  378  381  384  387  390  393  396  399  402  405  408  411  414  417\n",
            "  420  423  426  429  432  435  438  441  444  447  450  453  456  459\n",
            "  462  465  468  471  474  477  480  483  486  489  492  495  498  501\n",
            "  504  507  510  513  516  519  522  525  528  531  534  537  540  543\n",
            "  546  549  552  555  558  561  564  567  570  573  576  579  582  585\n",
            "  588  591  594  597  600  603  606  609  612  615  618  621  624  627\n",
            "  630  633  636  639  642  645  648  651  654  657  660  663  666  669\n",
            "  672  675  678  681  684  687  690  693  696  699  702  705  708  711\n",
            "  714  717  720  723  726  729  732  735  738  741  744  747  750  753\n",
            "  756  759  762  765  768  771  774  777  780  783  786  789  792  795\n",
            "  798  801  804  807  810  813  816  819  822  825  828  831  834  837\n",
            "  840  843  846  849  852  855  858  861  864  867  870  873  876  879\n",
            "  882  885  888  891  894  897  900  903  906  909  912  915  918  921\n",
            "  924  927  930  933  936  939  942  945  948  951  954  957  960  963\n",
            "  966  969  972  975  978  981  984  987  990  993  996  999 1002 1005\n",
            " 1008 1011 1014 1017 1020 1023 1026 1029 1032 1035 1038 1041 1044 1047\n",
            " 1050 1053 1056 1059 1062 1065 1068 1071 1074 1077 1080 1083 1086 1089\n",
            " 1092 1095 1098 1101 1104 1107 1110 1113 1116 1119 1122 1125 1128 1131\n",
            " 1134 1137 1140 1143 1146 1149 1152 1155 1158 1161 1164 1167 1170 1173\n",
            " 1176 1179 1182 1185 1188 1191 1194 1197 1200 1203 1206 1209 1212 1215\n",
            " 1218 1221 1224 1227 1230 1233 1236 1239 1242 1245 1248 1251 1254 1257\n",
            " 1260 1263 1266 1269 1272 1275 1278 1281 1284 1287 1290 1293 1296 1299\n",
            " 1302 1305 1308 1311 1314 1317 1320 1323 1326 1329 1332 1335 1338 1341\n",
            " 1344 1347 1350 1353 1356 1359 1362 1365 1368 1371 1374 1377 1380 1383\n",
            " 1386 1389 1392 1395 1398 1401 1404 1407 1410 1413 1416 1419 1422 1425\n",
            " 1428 1431 1434 1437 1440 1443 1446 1449 1452 1455 1458 1461 1464 1467\n",
            " 1470 1473 1476 1479 1482 1485 1488 1491 1494 1497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 16 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiplicacion matrices\n"
      ],
      "metadata": {
        "id": "hd43d3707gc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "@cuda.jit\n",
        "def mult_matrices(a, b, c):\n",
        "  ren, col = cuda.grid(2)\n",
        "  if ren < c.shape[0] and col < c.shape[1]:\n",
        "    suma = 0\n",
        "    for i in range(a.shape[1]):\n",
        "      suma += a[ren][i] * b[i][col]\n",
        "    c[ren][col] = suma\n",
        "\n",
        "block = (16, 16)\n",
        "a = np.full((32, 12), 2, float)\n",
        "b = np.full((12, 48), 4, float)\n",
        "c = np.full((32, 48), 0, float)\n",
        "a_device = cuda.to_device(a)\n",
        "b_device = cuda.to_device(b)\n",
        "c_device = cuda.device_array((32, 48))\n",
        "grid_x = int(math.ceil(a.shape[0]/block[0]))\n",
        "grid_y = int(math.ceil(b.shape[1]/block[1]))\n",
        "\n",
        "mult_matrices[(grid_x, grid_y), block](a_device, b_device, c_device)\n",
        "c = c_device.copy_to_host()\n",
        "print(c)"
      ],
      "metadata": {
        "id": "gCecKVCz7gCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53c4a98-0479-4ad6-edb3-edaedb2208f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 6 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[96. 96. 96. ... 96. 96. 96.]\n",
            " [96. 96. 96. ... 96. 96. 96.]\n",
            " [96. 96. 96. ... 96. 96. 96.]\n",
            " ...\n",
            " [96. 96. 96. ... 96. 96. 96.]\n",
            " [96. 96. 96. ... 96. 96. 96.]\n",
            " [96. 96. 96. ... 96. 96. 96.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtro en imagen de Aguila"
      ],
      "metadata": {
        "id": "TTMJRn363w8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import data, io\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def conv(pixels, output, size):\n",
        "    row, col = cuda.grid(2)\n",
        "    if (row + 2) < size[0] and (col + 2) < size[1] and row > 2 and col > 2:\n",
        "      divisor = 75\n",
        "      sum = pixels[row-2][col-1]/divisor + pixels[row-2][col]/divisor + pixels[row-2][col+1]/divisor + \\\n",
        "            pixels[row-1][col-1]/divisor + pixels[row-1][col]/divisor + pixels[row-1][col+1]/divisor + \\\n",
        "            pixels[row-0][col-1]/divisor + pixels[row-0][col]/divisor + pixels[row-0][col+1]/divisor + \\\n",
        "            pixels[row+1][col-1]/divisor + pixels[row+1][col]/divisor + pixels[row+1][col+1]/divisor + \\\n",
        "            pixels[row+2][col-1]/divisor + pixels[row+2][col]/divisor + pixels[row+2][col+1]/divisor\n",
        "      output[row][col] = round(sum/15)\n",
        "\n",
        "\n",
        "#eagle = data.eagle().astype(np.float32)\n",
        "eagle = io.imread(\"eagle.png\").astype(np.float32)\n",
        "print(eagle.min(), eagle.max(), eagle)\n",
        "plt.imshow(eagle, cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "##(2019, 1826)\n",
        "host_pixels = eagle\n",
        "device_pixels = cuda.to_device(host_pixels)\n",
        "device_output = cuda.device_array(host_pixels.shape)\n",
        "\n",
        "\n",
        "threads_per_block = (16, 16)\n",
        "blocks_x = math.ceil(eagle.shape[0] / threads_per_block[0])\n",
        "blocks_y = math.ceil(eagle.shape[1] / threads_per_block[1])\n",
        "grid = (blocks_x, blocks_y)\n",
        "\n",
        "conv[grid, threads_per_block](device_pixels, device_output, eagle.shape)\n",
        "plt.imshow(device_output.copy_to_host(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MtTj-X8b3uCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}